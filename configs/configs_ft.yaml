General:
  seed: 0
  num_devices: -1
  check_val_every_n_epoch: 1 # number of epochs between validation check, needs to be empty (None) for using #batches
  val_check_interval: # number of training batches between each validation check
  num_sanity_val_steps: 0
  log_every_n_steps: False
  fast_dev_run: False
  profiling: False
  inference_mode: True
  version:  # could be either a string or None, in which case the version is automatically generated


Model:
  model_type: expert # could be either 'analytics' or 'expert' or 'jpeg' or 'moe' or 'fusion'
  classifier: mislnet # 'mislnet' or 'resnet50'
  expert_ckpt: logs/ft_expert_mislnet_dn-real-500_dn-dalle2_src_q99/version_1/checkpoints/epoch=149-step=2400-v_loss=0.0647-v_acc=0.9780-last.ckpt
  analytics_ckpt: logs/ft_analytics_dn-real_du-gan_dn-sd_jpeg_q99/version_0/checkpoints/epoch=19-step=940-v_loss=0.2655-v_acc=0.8892.ckpt
  moe_ckpt: logs/moe_dn-real-500_dn-gan-500_dn-tt_dn-sd-500_dn-eg3d_dn-dalle2_q99/version_0/checkpoints/epoch=149-step=2400-v_loss=0.1274-v_acc=0.9522-last.ckpt
  expert_n_features: 200
  fusion_rule: 'max' # could be either 'avg' or 'max'
  fine_tune: True # if True, the model is fine-tuned from a checkpoint
  override_configs: True # if True, the config is overridden with the values in this file, otherwise the config is loaded from the checkpoint
  expert_task: src # could be either 'src' or 'manipulation', 'src_test_with_manipulation'
  src_ckpts: [
    # logs/expert_mislnet_db-real_db-gan_src_q99/version_1/checkpoints/epoch=32-step=120681-v_loss=0.0376-v_acc=0.9874.ckpt,
    # logs/ft_expert_mislnet_dn-real-500_dn-tt_src_q99/version_0/checkpoints/epoch=149-step=2400-v_loss=0.1243-v_acc=0.9560-last.ckpt,
    # logs/ft_expert_mislnet_dn-real-500_dn-sd-500_src_q99/version_0/checkpoints/epoch=149-step=2400-v_loss=0.3877-v_acc=0.8630-last.ckpt,
    # logs/ft_expert_mislnet_dn-real-500_dn-eg3d_src_q99/version_0/checkpoints/epoch=149-step=2400-v_loss=0.0127-v_acc=0.9950.ckpt,
    # logs/ft_expert_mislnet_dn-real-500_dn-dalle2_src_q99/version_0/checkpoints/epoch=149-step=2400-v_loss=0.0837-v_acc=0.9740-last.ckpt
  ] # list of paths to source classifier checkpoints to be used for analytics model
  manipulation_ckpts: [
    logs/ft_expert_resnet50_dn-real_dn-sd_src_q99/version_0/checkpoints/epoch=59-step=1920-v_loss=0.2927-v_acc=0.8750.ckpt
  ] # list of paths to manipulation classifier checkpoints to be used for analytics model
  fusion_ckpt: 
  jpeg_ckpt: logs/jpeg_midb_q50-100/version_0/checkpoints/epoch=96-step=213885-v_loss=2.5862-v_rmse=0.0000-v_mae=1.7335.ckpt
  analytics_manipulations: ['jpeg'] # list of manipulation names: 'unsharpmask', 'upsample', 'medianblur', 'bilateralblur' only used with analytics model
  expert_manipulation: ['medianblur'] # only used with expert model
  patch_size: 256

Data:
  datasets: [
  ]
  num_src_classes: 2
  train_txt_paths: [
  ]
  val_txt_paths: [
  ]
  test_txt_paths: [
  ]
  train_hdf5_paths: [/media/nas2/misl_image_db_70_class/experimental/256/train.hdf5, "", "", "", "", "", "", ""]
  val_hdf5_paths: [/media/nas2/misl_image_db_70_class/experimental/256/val.hdf5, "", "", "", "", "", "", ""]
  test_hdf5_paths: [/media/nas2/misl_image_db_70_class/experimental/256/test.hdf5, "", "", "", "", "", "", ""]
  jpeg_quality: [99] # only used for training, could be either an int or a list of ints
  test_jpeg_qualities: [99] # only used for testing with different jpeg qualities, should be a list of ints
  randomize_jpeg_quality: False # if True, jpeg_quality is ignored and it's set to list(np.arange(70, 100, 1))
  num_workers: 10
  prefetch_factor: 3

Train:
  epochs: 1
  max_steps: -1
  early_stopping: False
  loss_weights: [1.0, 1.0] # only used with mixture model loss to deal with class imbalance
  batch_size: 64
  lr: 1.0e-5
  src_loss_coeff: 1.0
  manipulation_loss_coeff: 0.0
  accumulate_grad_batches: 1
  scheduler: step # cosine, step
  lr_decay_rate: 0.9 # only used with step scheduler
  lr_step_size: 30 # only used with step scheduler
  min_lr: 1.0e-5 # only used with cosine scheduler
  optimizer: AdamW # Adam, AdamW, SGD
  momentum: 0.9 # only used with SGD optimizer
  train_dataset_hard_limit_num:
  val_dataset_hard_limit_num:
  test_dataset_hard_limit_num:
  train_dataset_limit_per_class: # class refers to either synthetic or real, synthetic is equally 
  val_dataset_limit_per_class: # disributed among the generators
  test_dataset_limit_per_class:
  train_dataset_limit: 1.0
  val_dataset_limit: 1.0
  test_dataset_limit: 1.0
  use_jit: False # if True, analytics model uses torch.jit.fork and torch.jit.wait for training