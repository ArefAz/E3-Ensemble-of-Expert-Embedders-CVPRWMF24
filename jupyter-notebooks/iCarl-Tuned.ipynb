{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75172c91-46d8-4974-8e60-c54baf0ae6e2",
   "metadata": {},
   "source": [
    "### From github repo: https://github.com/donlee90/icarl/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25b0738-1ef6-4e2a-97aa-618a8c7d811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "from pathlib import Path\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, txt_file_paths=None, transform=None, images_np=None, labels_np=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with either a list of .txt files containing image paths or numpy arrays of images and labels.\n",
    "        \n",
    "        Args:\n",
    "            txt_file_paths (list of str): List of paths to .txt files, each containing image paths. Each .txt file represents a class.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            images_np (numpy.ndarray, optional): Numpy array of images (used when not using .txt files for image paths).\n",
    "            labels_np (numpy.ndarray, optional): Numpy array of labels (used when not using .txt files for image paths).\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.images_np = images_np\n",
    "        self.labels_np = labels_np\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        if txt_file_paths is not None:\n",
    "            # Load image paths and labels from .txt files\n",
    "            for label, txt_path in enumerate(txt_file_paths):\n",
    "                with open(txt_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        self.image_paths.append(line.strip())  # Remove newline characters\n",
    "                        self.labels.append(label)  # The index of the .txt file is the label\n",
    "\n",
    "        self.from_numpy = images_np is not None and labels_np is not None\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.from_numpy:\n",
    "            return len(self.images_np)\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.from_numpy:\n",
    "            # Handle numpy array data\n",
    "            image = torch.from_numpy(self.images_np[idx])\n",
    "            label = self.labels_np[idx]\n",
    "            \n",
    "            # # Convert numpy image to PIL Image if necessary before applying transform\n",
    "            # if self.transform is not None:\n",
    "            #     image = Image.fromarray)(image)\n",
    "            #     image = self.transform(image)\n",
    "        else:\n",
    "            # Load image and label from the list populated from .txt files\n",
    "            img_path = self.image_paths[idx]\n",
    "            label = self.labels[idx]\n",
    "            image = Image.open(img_path)  # Assuming these are paths to images\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "        return idx, image, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4ec23a8-5cf3-427e-95b7-b398018be3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Hyper Parameters\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "class iCaRLNet(nn.Module):\n",
    "    def __init__(self, feature_size, n_classes):\n",
    "        # Network architecture\n",
    "        super(iCaRLNet, self).__init__()\n",
    "        self.feature_extractor = resnet18(weights=True)\n",
    "        self.feature_extractor.fc = nn.Linear(self.feature_extractor.fc.in_features, feature_size)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(feature_size, momentum=0.01)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.fc = nn.Linear(feature_size, n_classes, bias=False)\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.n_known = 0\n",
    "\n",
    "        # List containing exemplar_sets\n",
    "        # Each exemplar_set is a np.array of N images\n",
    "        # with shape (N, C, H, W)\n",
    "        self.exemplar_sets = []\n",
    "\n",
    "        # Learning method\n",
    "        self.cls_loss = nn.CrossEntropyLoss()\n",
    "        self.dist_loss = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate,\n",
    "                                    weight_decay=0.00001)\n",
    "        #self.optimizer = optim.SGD(self.parameters(), lr=2.0,\n",
    "        #                           weight_decay=0.00001)\n",
    "\n",
    "        # Means of exemplars\n",
    "        self.compute_means = True\n",
    "        self.exemplar_means = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.ReLU(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def increment_classes(self, n):\n",
    "        \"\"\"Add n classes in the final fc layer\"\"\"\n",
    "        in_features = self.fc.in_features\n",
    "        out_features = self.fc.out_features\n",
    "        weight = self.fc.weight.data\n",
    "\n",
    "        self.fc = nn.Linear(in_features, out_features+n, bias=False)\n",
    "        self.fc.weight.data[:out_features] = weight\n",
    "        self.n_classes += n\n",
    "\n",
    "    def classify(self, x, transform):\n",
    "        \"\"\"Classify images by neares-means-of-exemplars\n",
    "\n",
    "        Args:\n",
    "            x: input image batch\n",
    "        Returns:\n",
    "            preds: Tensor of size (batch_size,)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        if self.compute_means:\n",
    "            print (\"Computing mean of exemplars...\")\n",
    "            exemplar_means = []\n",
    "            for P_y in self.exemplar_sets:\n",
    "                features = []\n",
    "                # Extract feature for each exemplar in P_y\n",
    "                for ex in P_y:\n",
    "                    ex = Variable(torch.from_numpy(ex), volatile=True).cuda()\n",
    "                    feature = self.feature_extractor(ex.unsqueeze(0))\n",
    "                    feature = feature.squeeze()\n",
    "                    feature.data = feature.data / feature.data.norm() # Normalize\n",
    "                    features.append(feature)\n",
    "                features = torch.stack(features)\n",
    "                mu_y = features.mean(0).squeeze()\n",
    "                mu_y.data = mu_y.data / mu_y.data.norm() # Normalize\n",
    "                exemplar_means.append(mu_y)\n",
    "            self.exemplar_means = exemplar_means\n",
    "            self.compute_means = False\n",
    "            print(\"Done\")\n",
    "\n",
    "        exemplar_means = self.exemplar_means\n",
    "        means = torch.stack(exemplar_means) # (n_classes, feature_size)\n",
    "        means = torch.stack([means] * batch_size) # (batch_size, n_classes, feature_size)\n",
    "        means = means.transpose(1, 2) # (batch_size, feature_size, n_classes)\n",
    "\n",
    "        feature = self.feature_extractor(x) # (batch_size, feature_size)\n",
    "        for i in range(feature.size(0)): # Normalize\n",
    "            feature.data[i] = feature.data[i] / feature.data[i].norm()\n",
    "        feature = feature.unsqueeze(2) # (batch_size, feature_size, 1)\n",
    "        \n",
    "        feature = feature.expand_as(means) # (batch_size, feature_size, n_classes)\n",
    "\n",
    "        dists = (feature - means).pow(2).sum(1).squeeze() #(batch_size, n_classes)\n",
    "        _, preds = dists.min(1)\n",
    "\n",
    "        return preds\n",
    "        \n",
    "\n",
    "    def construct_exemplar_set(self, dataset,  m, transform):  \n",
    "        # ***************************************************************\n",
    "        # Constructing exemplar set needs to be done through dataloader \n",
    "        # Passing all the images just to create 'm' exemplar set is not good.\n",
    "        # Get exemplar_set and exemplar_feature only fit 'm' new exemplars so only best m fit. Sort by some similarity metric?\n",
    "        # ***************************************************************\n",
    "        \n",
    "        \"\"\"Construct an exemplar set for image set\n",
    "\n",
    "        Args:\n",
    "            dataset: This dataset will have images from which exemplar set will be created\n",
    "        \"\"\"\n",
    "        # Compute and cache features for each example\n",
    "        features = []\n",
    "\n",
    "        features = []\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            # Assuming batch[0] is a batch of images\n",
    "            imgs = batch[1]\n",
    "            imgs = imgs.cuda()\n",
    "\n",
    "            # print(imgs.shape)\n",
    "\n",
    "            with torch.no_grad():  # Use torch.no_grad() instead of volatile\n",
    "                feature = self.feature_extractor(imgs).cpu().numpy()\n",
    "                feature = feature / np.linalg.norm(feature, axis=1, keepdims=True)  # Normalize\n",
    "                features.extend(feature)\n",
    "        \n",
    "        # for img in images:\n",
    "        #     # x = Variable(transform(Image.fromarray(img)), volatile=True).cuda()\n",
    "        #     x = Variable(transform(img), volatile=True).cuda()\n",
    "        #     feature = self.feature_extractor(x.unsqueeze(0)).data.cpu().numpy()\n",
    "        #     feature = feature / np.linalg.norm(feature) # Normalize\n",
    "        #     features.append(feature[0])\n",
    "\n",
    "        features = np.array(features)\n",
    "        class_mean = np.mean(features, axis=0)\n",
    "        class_mean = class_mean / np.linalg.norm(class_mean) # Normalize\n",
    "\n",
    "        exemplar_set = []\n",
    "        exemplar_features = [] # list of Variables of shape (feature_size,)\n",
    "        for k in range(m):\n",
    "            S = np.sum(exemplar_features, axis=0)\n",
    "            phi = features\n",
    "            mu = class_mean\n",
    "            mu_p = 1.0/(k+1) * (phi + S)\n",
    "            mu_p = mu_p / np.linalg.norm(mu_p)\n",
    "            i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n",
    " \n",
    "            exemplar_set.append(dataset[i][1])   # 0th index is index, 1st is image, 2nd is label\n",
    "            exemplar_features.append(features[i])\n",
    "            \n",
    "            \"\"\"\n",
    "            print (\"Selected example\", i)\n",
    "            print (\"|exemplar_mean - class_mean|:\")\n",
    "            print (np.linalg.norm((np.mean(exemplar_features, axis=0) - class_mean))\n",
    "            #features = np.delete(features, i, axis=0)\n",
    "            \"\"\"\n",
    "        \n",
    "        self.exemplar_sets.append(np.array(exemplar_set))\n",
    "                \n",
    "\n",
    "    def reduce_exemplar_sets(self, m):\n",
    "        for y, P_y in enumerate(self.exemplar_sets):\n",
    "            self.exemplar_sets[y] = P_y[:m]\n",
    "\n",
    "\n",
    "    def combine_dataset_with_exemplars(self, dataset):\n",
    "        # ***************************************************************\n",
    "        # This too needs to be done with data loader.. Why load all the dataset into memory just to append?\n",
    "        # Rather create a dataset with exemplar images + exemplar labels and then union them.\n",
    "        # ***************************************************************\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "                Resize((128, 128)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        \n",
    "        for y, P_y in enumerate(self.exemplar_sets):\n",
    "            exemplar_images = P_y\n",
    "            exemplar_labels = [y] * len(P_y)\n",
    "        \n",
    "            exemplar_dataset = CustomDataset(images_np=exemplar_images, labels_np=exemplar_labels, transform=transform)\n",
    "\n",
    "            # Create dataset with numpy arrays exemplar_images, exemplar_labels\n",
    "            dataset = ConcatDataset([dataset, exemplar_dataset])\n",
    "\n",
    "        return dataset\n",
    "            \n",
    "\n",
    "\n",
    "    def update_representation(self, dataset):\n",
    "        # ***************************************************************\n",
    "        # Pass dataloader not dataset.. Creating  new loader inside is not necessary.\n",
    "        # ***************************************************************\n",
    "        \n",
    "        self.compute_means = True\n",
    "\n",
    "        # Increment number of weights in final fc layer\n",
    "        classes = list(set(dataset.labels))\n",
    "        new_classes = [cls for cls in classes if cls > self.n_classes - 1]\n",
    "        self.increment_classes(len(new_classes))\n",
    "        self.cuda()\n",
    "        print (\"%d new classes\" % (len(new_classes)))\n",
    "\n",
    "        # Form combined training set\n",
    "        dataset = self.combine_dataset_with_exemplars(dataset)\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                               shuffle=True, num_workers=2)\n",
    "\n",
    "        # Store network outputs with pre-update parameters\n",
    "        q = torch.zeros(len(dataset), self.n_classes).cuda()\n",
    "        for indices, images, labels in loader:\n",
    "            images = Variable(images).cuda()\n",
    "            indices = indices.cuda()\n",
    "            g = F.sigmoid(self.forward(images))\n",
    "            q[indices] = g.data\n",
    "        q = Variable(q).cuda()\n",
    "\n",
    "        # Run network training\n",
    "        optimizer = self.optimizer\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (indices, images, labels) in enumerate(loader):\n",
    "                images = Variable(images).cuda()\n",
    "                labels = Variable(labels).cuda()\n",
    "                indices = indices.cuda()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                g = self.forward(images)\n",
    "                \n",
    "                # Classification loss for new classes\n",
    "                loss = self.cls_loss(g, labels)\n",
    "                #loss = loss / len(range(self.n_known, self.n_classes))\n",
    "\n",
    "                # Distilation loss for old classes\n",
    "                if self.n_known > 0:\n",
    "                    g = F.sigmoid(g)\n",
    "                    q_i = q[indices]\n",
    "                    dist_loss = sum(self.dist_loss(g[:,y], q_i[:,y])\\\n",
    "                            for y in range(self.n_known))\n",
    "                    #dist_loss = dist_loss / self.n_known\n",
    "                    loss += dist_loss\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                           %(epoch+1, num_epochs, i+1, len(dataset)//batch_size, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "845db1fc-2eec-4346-9243-919b39100a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Scenario 1: Real vs GAN\n",
    "txt_file_paths = ['data/real_train_filepaths.txt', 'data/gan_train_filepaths.txt']  \n",
    "train_dataset = CustomDataset(txt_file_paths=txt_file_paths, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "txt_file_paths = ['data/real_test_filepaths.txt', 'data/gan_test_filepaths.txt']  \n",
    "test_dataset = CustomDataset(txt_file_paths=txt_file_paths, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4eedbbf-211b-4b1e-ac97-237b5745ec31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(train_dataset.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbfc9289-3d58-4c01-b1ad-97586fd39eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ms5267@drexel.edu/synthetic-image-detection/ve-m/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/ms5267@drexel.edu/synthetic-image-detection/ve-m/lib/python3.10/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 new classes\n",
      "Epoch [1/5], Iter [10/50] Loss: 0.6627\n",
      "Epoch [1/5], Iter [20/50] Loss: 0.6980\n",
      "Epoch [1/5], Iter [30/50] Loss: 0.7042\n",
      "Epoch [1/5], Iter [40/50] Loss: 0.7007\n",
      "Epoch [1/5], Iter [50/50] Loss: 0.7115\n",
      "Epoch [2/5], Iter [10/50] Loss: 0.7120\n",
      "Epoch [2/5], Iter [20/50] Loss: 0.7361\n",
      "Epoch [2/5], Iter [30/50] Loss: 0.6916\n",
      "Epoch [2/5], Iter [40/50] Loss: 0.6848\n",
      "Epoch [2/5], Iter [50/50] Loss: 0.6975\n",
      "Epoch [3/5], Iter [10/50] Loss: 0.6688\n",
      "Epoch [3/5], Iter [20/50] Loss: 0.6250\n",
      "Epoch [3/5], Iter [30/50] Loss: 0.5741\n",
      "Epoch [3/5], Iter [40/50] Loss: 0.4296\n",
      "Epoch [3/5], Iter [50/50] Loss: 0.2612\n",
      "Epoch [4/5], Iter [10/50] Loss: 0.4466\n",
      "Epoch [4/5], Iter [20/50] Loss: 0.4430\n",
      "Epoch [4/5], Iter [30/50] Loss: 0.6446\n",
      "Epoch [4/5], Iter [40/50] Loss: 0.3206\n",
      "Epoch [4/5], Iter [50/50] Loss: 0.4531\n",
      "Epoch [5/5], Iter [10/50] Loss: 0.2847\n",
      "Epoch [5/5], Iter [20/50] Loss: 0.3683\n",
      "Epoch [5/5], Iter [30/50] Loss: 0.4499\n",
      "Epoch [5/5], Iter [40/50] Loss: 0.4907\n",
      "Epoch [5/5], Iter [50/50] Loss: 0.3636\n"
     ]
    }
   ],
   "source": [
    "# Initialize CNN\n",
    "K = 2000 # total number of exemplars\n",
    "icarl = iCaRLNet(2048, 0)\n",
    "icarl.cuda()\n",
    "\n",
    "icarl.update_representation(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f858c1-699c-46e6-b92c-d2a099029393",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = int(K / icarl.n_classes)\n",
    "icarl.reduce_exemplar_sets(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32991617-0e37-4dc3-8fff-d359541a7cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b5d85d0-2fe4-43bf-b6d7-28437e962fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing exemplar set for class-0...\n",
      "Done\n",
      "Constructing exemplar set for class-1...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Construct exemplar sets for new classes\n",
    "for y in range(icarl.n_known, icarl.n_classes):\n",
    "    print(\"Constructing exemplar set for class-%d...\" %(y))\n",
    "    # images = train_dataset.get_image_class(y).transpose(0,3,2,1)\n",
    "    # Need subset of train_dataset\n",
    "    # Identify indices of samples with the desired label\n",
    "    filtered_indices = [i for i, (idx, _, label) in enumerate(train_dataset) if label == y]\n",
    "    \n",
    "    subset_dataset = Subset(train_dataset, filtered_indices)\n",
    "    \n",
    "    icarl.construct_exemplar_set(subset_dataset, m, transform_test)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83c384cd-1bb9-43bd-b337-58a2ef14b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplar set for class-0: (1000, 3, 128, 128)\n",
      "Exemplar set for class-1: (1000, 3, 128, 128)\n",
      "iCaRL classes: 2\n"
     ]
    }
   ],
   "source": [
    "for y, P_y in enumerate(icarl.exemplar_sets):\n",
    "    print(\"Exemplar set for class-%d:\" % (y), P_y.shape)\n",
    "    #show_images(P_y[:10])\n",
    "\n",
    "icarl.n_known = icarl.n_classes\n",
    "print(\"iCaRL classes: %d\" % icarl.n_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e92507-d4fd-4821-bc15-55cd13bfddd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean of exemplars...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46131/3328298797.py:71: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  ex = Variable(torch.from_numpy(ex), volatile=True).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Train Accuracy: 35.937500 %\n"
     ]
    }
   ],
   "source": [
    "total = 0.0\n",
    "correct = 0.0\n",
    "\n",
    "for idx,images, labels in train_dataloader:\n",
    "    images = Variable(images).cuda()\n",
    "    # print(type(images), images.shape)\n",
    "    preds = icarl.classify(images, transform)\n",
    "    total += labels.size(0)\n",
    "    correct += (preds.data.cpu() == labels).sum()\n",
    "\n",
    "print('Train Accuracy: %f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8f174fa-bd2a-4148-8c0f-62fb4882c3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "         1, 0, 1, 0, 1, 1, 1, 1], device='cuda:0'),\n",
       " tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d782d9-5000-465e-9da7-6df38386d174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600.0, tensor(575.))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8183971-77d6-4000-ab7e-bb0edbf261d6",
   "metadata": {},
   "source": [
    "#### Now, create dataset for diffusion model \n",
    "and run the iCarl to see how well it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f02fbeed-c02c-4a67-9a1f-4a2b77fe20ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Real vs GAN\n",
    "txt_file_paths = ['data/real_train_filepaths.txt', 'data/diffusion_train_filepaths.txt']  \n",
    "train_dataset = CustomDataset(txt_file_paths=txt_file_paths, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "txt_file_paths = ['data/real_test_filepaths.txt', 'data/diffusion_test_filepaths.txt']  \n",
    "test_dataset = CustomDataset(txt_file_paths=txt_file_paths, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "879beb6d-ca16-4be4-b4cc-ea5678be158c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " tensor([[[ 0.3430, -0.1416, -0.0447,  ...,  0.7501,  0.6531,  0.7113],\n",
       "          [ 0.6725,  0.2073,  0.0910,  ...,  0.7501,  0.7501,  0.7694],\n",
       "          [ 0.9439,  0.5562, -0.0447,  ...,  0.4399,  0.6531,  0.7307],\n",
       "          ...,\n",
       "          [-0.3743, -0.4906, -0.7426,  ..., -1.5567, -1.5567, -1.5374],\n",
       "          [-0.8977, -0.7813, -0.7426,  ..., -1.5761, -1.5761, -1.5761],\n",
       "          [-1.0140, -0.7426, -0.7426,  ..., -1.6149, -1.6149, -1.5955]],\n",
       " \n",
       "         [[-0.4712, -0.8842, -0.4122,  ...,  0.1188,  0.2368,  0.3941],\n",
       "          [-0.1762, -0.5499, -0.2942,  ...,  0.0598,  0.1384,  0.2171],\n",
       "          [ 0.0401, -0.1959, -0.3336,  ..., -0.3336, -0.1762, -0.0582],\n",
       "          ...,\n",
       "          [-0.4516, -0.3532, -0.4712,  ..., -1.5529, -1.5529, -1.5332],\n",
       "          [-1.0416, -0.7662, -0.5499,  ..., -1.5726, -1.5726, -1.5726],\n",
       "          [-1.0612, -0.7072, -0.6089,  ..., -1.6119, -1.6119, -1.5922]],\n",
       " \n",
       "         [[-0.4850, -1.0898, -0.6606,  ..., -0.0362,  0.1003,  0.2564],\n",
       "          [-0.1338, -0.6606, -0.4264,  ..., -0.2118, -0.1143,  0.0223],\n",
       "          [ 0.0418, -0.1533, -0.2313,  ..., -0.6411, -0.4850, -0.2704],\n",
       "          ...,\n",
       "          [-0.4460, -0.5240, -0.7191,  ..., -1.4605, -1.4605, -1.4410],\n",
       "          [-1.1288, -0.9727, -0.8362,  ..., -1.4800, -1.4800, -1.4800],\n",
       "          [-1.2459, -0.9922, -0.9727,  ..., -1.4995, -1.5190, -1.4995]]]),\n",
       " 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "710a51f1-63d6-4bcd-b64e-7a6346f51f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "### Define dataset for diffusion model data\n",
    "for indices, images, labels in train_dataloader:\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fac0e3d-782a-4f44-a985-2193f52fdb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 new classes\n",
      "Epoch [1/5], Iter [10/112] Loss: 1.9632\n",
      "Epoch [1/5], Iter [20/112] Loss: 1.9495\n",
      "Epoch [1/5], Iter [30/112] Loss: 1.9548\n",
      "Epoch [1/5], Iter [40/112] Loss: 2.0165\n",
      "Epoch [1/5], Iter [50/112] Loss: 1.8742\n",
      "Epoch [1/5], Iter [60/112] Loss: 1.9543\n",
      "Epoch [1/5], Iter [70/112] Loss: 1.8636\n",
      "Epoch [1/5], Iter [80/112] Loss: 1.9186\n",
      "Epoch [1/5], Iter [90/112] Loss: 2.0417\n",
      "Epoch [1/5], Iter [100/112] Loss: 1.9725\n",
      "Epoch [1/5], Iter [110/112] Loss: 2.0107\n",
      "Epoch [2/5], Iter [10/112] Loss: 2.0366\n",
      "Epoch [2/5], Iter [20/112] Loss: 2.0058\n",
      "Epoch [2/5], Iter [30/112] Loss: 1.9491\n",
      "Epoch [2/5], Iter [40/112] Loss: 2.0852\n",
      "Epoch [2/5], Iter [50/112] Loss: 1.7597\n",
      "Epoch [2/5], Iter [60/112] Loss: 1.8620\n",
      "Epoch [2/5], Iter [70/112] Loss: 1.8633\n",
      "Epoch [2/5], Iter [80/112] Loss: 1.8836\n",
      "Epoch [2/5], Iter [90/112] Loss: 1.8795\n",
      "Epoch [2/5], Iter [100/112] Loss: 1.8343\n",
      "Epoch [2/5], Iter [110/112] Loss: 1.9384\n",
      "Epoch [3/5], Iter [10/112] Loss: 1.8167\n",
      "Epoch [3/5], Iter [20/112] Loss: 1.7740\n",
      "Epoch [3/5], Iter [30/112] Loss: 1.8621\n",
      "Epoch [3/5], Iter [40/112] Loss: 1.8834\n",
      "Epoch [3/5], Iter [50/112] Loss: 1.8403\n",
      "Epoch [3/5], Iter [60/112] Loss: 1.8241\n",
      "Epoch [3/5], Iter [70/112] Loss: 1.8858\n",
      "Epoch [3/5], Iter [80/112] Loss: 1.9797\n",
      "Epoch [3/5], Iter [90/112] Loss: 1.8044\n",
      "Epoch [3/5], Iter [100/112] Loss: 1.9567\n",
      "Epoch [3/5], Iter [110/112] Loss: 1.8767\n",
      "Epoch [4/5], Iter [10/112] Loss: 1.7760\n",
      "Epoch [4/5], Iter [20/112] Loss: 1.8722\n",
      "Epoch [4/5], Iter [30/112] Loss: 1.8239\n",
      "Epoch [4/5], Iter [40/112] Loss: 1.8457\n",
      "Epoch [4/5], Iter [50/112] Loss: 1.7836\n",
      "Epoch [4/5], Iter [60/112] Loss: 1.9353\n",
      "Epoch [4/5], Iter [70/112] Loss: 1.8190\n",
      "Epoch [4/5], Iter [80/112] Loss: 1.8538\n",
      "Epoch [4/5], Iter [90/112] Loss: 1.9339\n",
      "Epoch [4/5], Iter [100/112] Loss: 1.8837\n",
      "Epoch [4/5], Iter [110/112] Loss: 1.7828\n",
      "Epoch [5/5], Iter [10/112] Loss: 1.8006\n",
      "Epoch [5/5], Iter [20/112] Loss: 1.9089\n",
      "Epoch [5/5], Iter [30/112] Loss: 1.8787\n",
      "Epoch [5/5], Iter [40/112] Loss: 1.8665\n",
      "Epoch [5/5], Iter [50/112] Loss: 1.8609\n",
      "Epoch [5/5], Iter [60/112] Loss: 1.8754\n",
      "Epoch [5/5], Iter [70/112] Loss: 1.8220\n",
      "Epoch [5/5], Iter [80/112] Loss: 1.7027\n",
      "Epoch [5/5], Iter [90/112] Loss: 1.9034\n",
      "Epoch [5/5], Iter [100/112] Loss: 1.7638\n",
      "Epoch [5/5], Iter [110/112] Loss: 1.8081\n"
     ]
    }
   ],
   "source": [
    "icarl.update_representation(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8510c1ed-751f-4b1c-9599-1a6760c55e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 128, 128]), torch.Size([3, 128, 128]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = icarl.combine_dataset_with_exemplars(train_dataset)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                       shuffle=True, num_workers=2)\n",
    "\n",
    "dataset[3500][1].shape, dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d23e2007-6a3b-4c88-962b-163858967b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = int(K / icarl.n_classes)\n",
    "\n",
    "# Reduce exemplar sets for known classes\n",
    "icarl.reduce_exemplar_sets(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23796d9a-3c87-4650-bf16-23b03398fd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemplar set for class-0: (1000, 3, 128, 128)\n",
      "Exemplar set for class-1: (1000, 3, 128, 128)\n",
      "iCaRL classes: 2\n"
     ]
    }
   ],
   "source": [
    "# Construct exemplar sets for new classes\n",
    "for y in range(icarl.n_known, icarl.n_classes):\n",
    "    print (\"Constructing exemplar set for class-%d...\" %(y))\n",
    "    images = train_set.get_image_class(y)\n",
    "    icarl.construct_exemplar_set(images, m, transform_test)\n",
    "    print (\"Done\")\n",
    "\n",
    "for y, P_y in enumerate(icarl.exemplar_sets):\n",
    "    print (\"Exemplar set for class-%d:\" % (y), P_y.shape)\n",
    "    #show_images(P_y[:10])\n",
    "\n",
    "icarl.n_known = icarl.n_classes\n",
    "print (\"iCaRL classes: %d\" % icarl.n_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ac5beb4-8965-41e5-ad83-7d3a39ca47f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean of exemplars...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46131/3328298797.py:71: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  ex = Variable(torch.from_numpy(ex), volatile=True).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Train Accuracy: 36 %\n"
     ]
    }
   ],
   "source": [
    "total = 0.0\n",
    "correct = 0.0\n",
    "for indices, images, labels in train_dataloader:\n",
    "    images = Variable(images).cuda()\n",
    "    preds = icarl.classify(images, transform_test)\n",
    "    total += labels.size(0)\n",
    "    correct += (preds.data.cpu() != labels).sum()\n",
    "\n",
    "print('Train Accuracy: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd972e9b-c2ac-4823-ba0e-6f87d5f7589f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
